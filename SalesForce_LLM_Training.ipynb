{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgangoly/Generative-AI-for-Security/blob/main/SalesForce_LLM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90fc8744",
      "metadata": {
        "id": "90fc8744"
      },
      "source": [
        "## LLM with Salesforce/xgen-7b-8k-base model, using PyTorch and Alpaca dataset from Hugging Face ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef08f50",
      "metadata": {
        "id": "8ef08f50"
      },
      "source": [
        "Trl: used to train transformer language models with reinforcement learning.\n",
        "\n",
        "<br> Peft uses the parameter-efficient fine-tuning (PEFT) methods to enable efficient adaptation of the pre-trained model.\n",
        "<br>\n",
        "Torch: a widely-used open-source machine learning library.\n",
        "<br>\n",
        "Datasets: used to assist in downloading and loading many common machine learning datasets.\n",
        "<br>\n",
        "The code is written in a Jupyter notebook cell with the \"%%bash\", which runs the cell as a bash shell script.\n",
        "<br>\n",
        "\"pip -q install trl\" ***installs the 'trl'*** Python package quietly, meaning it won't print any output.\n",
        "<br>\n",
        "\"pip -q install peft\" ***installs the 'peft'*** Python package quietly.\n",
        "<br>\n",
        "\"pip -q install torch\" ***installs the 'torch'*** Python package quietly, which is a popular library for deep learning.\n",
        "<br>\n",
        "\"pip -q install datasets\" ***installs the 'datasets'*** Python package quietly, used for loading and processing datasets.\n",
        "<br>\n",
        "\"pip -q install transformers\" ***installs the 'transformers'*** Python package quietly, used for state-of-the-art natural language processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f4b5e9",
      "metadata": {
        "id": "a4f4b5e9",
        "outputId": "116633cb-07e5-4caf-9eef-e89d1d8ed55e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: /home/sgangoly/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by bash)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip -q install trl\n",
        "pip -q install peft\n",
        "pip -q install torch\n",
        "pip -q install datasets\n",
        "pip -q install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8267d954",
      "metadata": {
        "id": "8267d954"
      },
      "source": [
        "Import the ***torch*** library, a popular open-source machine learning library.\n",
        "<br>\n",
        "• from trl import ***SFTTrainer*** imports the SFTTrainer class from the trl (Transfer Reinforcement Learning) library.\n",
        "<br>\n",
        "• from datasets import ***load_dataset*** imports the load_dataset function from the datasets library.\n",
        "<br>\n",
        "• from peft import ***LoraConfig, get_peft_model, prepare_model_for_int8_training*** imports three items from the peft library.\n",
        "<br>\n",
        "• from transformers import ***AutoModelForCausalLM, AutoTokenizer, TrainingArguments*** imports three items from the transformers library, a popular library for Natural Language Processing (NLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1c82757",
      "metadata": {
        "id": "b1c82757"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dadfe9",
      "metadata": {
        "id": "79dadfe9"
      },
      "source": [
        "### Data Loading and Preparation\n",
        "\n",
        "The  ***alpaca***  dataset, freely available on ***Hugging Face***, is used. The dataset has three main columns: ***instructions, input***, and ***output***. These columns are combined to generate a final text column.\n",
        "\n",
        "The instruction to load the dataset is given below by providing the name of the dataset of interest, which is ***tatsu-lab/alpaca***\n",
        "\n",
        "<br>\n",
        "\n",
        "The code is written in Python and uses the load_dataset function from the Hugging Face's datasets library.\n",
        "<br>\n",
        "The load_dataset function is used to load a dataset for machine learning tasks.\n",
        "<br>\n",
        "\"tatsu-lab/alpaca\" is the identifier of the dataset to be loaded from the Hugging Face's dataset hub.\n",
        "<br>\n",
        "The split parameter is set to \"train\" to load only the training portion of the dataset.\n",
        "<br>\n",
        "The loaded dataset is stored in the train_dataset variable.\n",
        "<br>\n",
        "The print function is used to display the content of the train_dataset variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2793ab3b",
      "metadata": {
        "id": "2793ab3b",
        "outputId": "cc31ec81-f780-4c2c-e144-13d87a06ff89",
        "colab": {
          "referenced_widgets": [
            "ca911e2fca3e4850b7aae9225bfdb8e7",
            "8d67ea4e2ba54f609306c6a255b6b3a3",
            "ce31d20c4aeb4f3fa8c8d1eec2756708"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca911e2fca3e4850b7aae9225bfdb8e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d67ea4e2ba54f609306c6a255b6b3a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce31d20c4aeb4f3fa8c8d1eec2756708",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'text'],\n",
            "    num_rows: 52002\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "train_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9ec8bf",
      "metadata": {
        "id": "0d9ec8bf"
      },
      "source": [
        "The data above is in a dictionary of two keys:\n",
        "<br>\n",
        "- **Features** : containing the main columns of the data\n",
        "<br>\n",
        "- **Num_rows**: corresponding to the total number of rows in the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "364e38d0",
      "metadata": {
        "id": "364e38d0"
      },
      "source": [
        "The first five rows are displayed. First, we convert the dictionary into a pandas dataframe, then display the rows.\n",
        "<br>\n",
        "- The first line converts the ***'train_dataset'*** to a pandas DataFrame using the ***'to_pandas()'*** method.\n",
        "- The second line uses the ***'display()'*** function to print the first 5 rows of the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac0a7fd",
      "metadata": {
        "id": "7ac0a7fd",
        "outputId": "0770217c-5932-4cc9-ea0d-e3471a3795bd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>instruction</th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Give three tips for staying healthy.</td>\n",
              "      <td></td>\n",
              "      <td>1.Eat a balanced diet and make sure to include...</td>\n",
              "      <td>Below is an instruction that describes a task....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are the three primary colors?</td>\n",
              "      <td></td>\n",
              "      <td>The three primary colors are red, blue, and ye...</td>\n",
              "      <td>Below is an instruction that describes a task....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Describe the structure of an atom.</td>\n",
              "      <td></td>\n",
              "      <td>An atom is made up of a nucleus, which contain...</td>\n",
              "      <td>Below is an instruction that describes a task....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How can we reduce air pollution?</td>\n",
              "      <td></td>\n",
              "      <td>There are a number of ways to reduce air pollu...</td>\n",
              "      <td>Below is an instruction that describes a task....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Describe a time when you had to make a difficu...</td>\n",
              "      <td></td>\n",
              "      <td>I had to make a difficult decision when I was ...</td>\n",
              "      <td>Below is an instruction that describes a task....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         instruction input  \\\n",
              "0               Give three tips for staying healthy.         \n",
              "1                 What are the three primary colors?         \n",
              "2                 Describe the structure of an atom.         \n",
              "3                   How can we reduce air pollution?         \n",
              "4  Describe a time when you had to make a difficu...         \n",
              "\n",
              "                                              output  \\\n",
              "0  1.Eat a balanced diet and make sure to include...   \n",
              "1  The three primary colors are red, blue, and ye...   \n",
              "2  An atom is made up of a nucleus, which contain...   \n",
              "3  There are a number of ways to reduce air pollu...   \n",
              "4  I had to make a difficult decision when I was ...   \n",
              "\n",
              "                                                text  \n",
              "0  Below is an instruction that describes a task....  \n",
              "1  Below is an instruction that describes a task....  \n",
              "2  Below is an instruction that describes a task....  \n",
              "3  Below is an instruction that describes a task....  \n",
              "4  Below is an instruction that describes a task....  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pandas_format = train_dataset.to_pandas()\n",
        "display(pandas_format.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b27b917",
      "metadata": {
        "id": "2b27b917"
      },
      "source": [
        "For better visualization, we print the information about the first three rows. Prior to doing that, we install the ***textwrap*** library to set the maximum number of words per line to 50. The first print statement separates each block by 15 dashes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb5c5ee",
      "metadata": {
        "id": "edb5c5ee"
      },
      "source": [
        "- The code begins with a for loop that iterates over a range of 3, meaning it will run three times.\n",
        "- For each iteration, it first prints a line of hyphens to separate the output of each loop.\n",
        "- It then prints the word \"Instruction:\" followed by the value of the \"instruction\" column of the pandas DataFrame at the current index.\n",
        "- The ***textwrap.fill*** function is used to wrap the text at a specified width, in this case, 50 characters.\n",
        "- It then prints the word \"Output:\" followed by the value of the \"output\" column of the DataFrame at the current index.\n",
        "- Again, ***textwrap.fill*** is used to wrap the text at a width of 50 characters.\n",
        "- Finally, it prints the word \"Text:\" followed by the value of the \"text\" column of the DataFrame at the current index.\n",
        "- The ***textwrap.fill*** function is used one last time to wrap the text at a width of 50 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b8cd59",
      "metadata": {
        "id": "d5b8cd59",
        "outputId": "0a3b531a-06cb-4822-fc5e-3e2d5815f223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/sgangoly/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n"
          ]
        }
      ],
      "source": [
        "!pip -q install textwrap3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a4b242",
      "metadata": {
        "id": "29a4b242"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf38d7d9",
      "metadata": {
        "id": "bf38d7d9",
        "outputId": "5c12ee9d-294f-4532-f042-6122b031bf6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------\n",
            "Instruction: Give three tips for staying healthy.\n",
            "Output: 1.Eat a balanced diet and make sure to include\n",
            "plenty of fruits and vegetables.  2. Exercise\n",
            "regularly to keep your body active and strong.  3.\n",
            "Get enough sleep and maintain a consistent sleep\n",
            "schedule.\n",
            "Text: Below is an instruction that describes a task.\n",
            "Write a response that appropriately completes the\n",
            "request.  ### Instruction: Give three tips for\n",
            "staying healthy.  ### Response: 1.Eat a balanced\n",
            "diet and make sure to include plenty of fruits and\n",
            "vegetables.  2. Exercise regularly to keep your\n",
            "body active and strong.  3. Get enough sleep and\n",
            "maintain a consistent sleep schedule.\n",
            "---------------------------------------------\n",
            "Instruction: What are the three primary colors?\n",
            "Output: The three primary colors are red, blue, and\n",
            "yellow.\n",
            "Text: Below is an instruction that describes a task.\n",
            "Write a response that appropriately completes the\n",
            "request.  ### Instruction: What are the three\n",
            "primary colors?  ### Response: The three primary\n",
            "colors are red, blue, and yellow.\n",
            "---------------------------------------------\n",
            "Instruction: Describe the structure of an atom.\n",
            "Output: An atom is made up of a nucleus, which contains\n",
            "protons and neutrons, surrounded by electrons that\n",
            "travel in orbits around the nucleus. The protons\n",
            "and neutrons have a positive charge, while the\n",
            "electrons have a negative charge, resulting in an\n",
            "overall neutral atom. The number of each particle\n",
            "determines the atomic number and the type of atom.\n",
            "Text: Below is an instruction that describes a task.\n",
            "Write a response that appropriately completes the\n",
            "request.  ### Instruction: Describe the structure\n",
            "of an atom.  ### Response: An atom is made up of a\n",
            "nucleus, which contains protons and neutrons,\n",
            "surrounded by electrons that travel in orbits\n",
            "around the nucleus. The protons and neutrons have\n",
            "a positive charge, while the electrons have a\n",
            "negative charge, resulting in an overall neutral\n",
            "atom. The number of each particle determines the\n",
            "atomic number and the type of atom.\n"
          ]
        }
      ],
      "source": [
        "for index in range(3):\n",
        "  print(\"---\"*15)\n",
        "  print(\"Instruction: {}\".format(textwrap.fill(pandas_format.iloc[index][\"instruction\"], width=50)))\n",
        "  print(\"Output: {}\".format(textwrap.fill(pandas_format.iloc[index][\"output\"], width=50)))\n",
        "  print(\"Text: {}\".format(textwrap.fill(pandas_format.iloc[index][\"text\"], width=50)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bafefe1b",
      "metadata": {
        "id": "bafefe1b"
      },
      "source": [
        "## Model Training ##\n",
        "\n",
        "Before proceeding to train the model, we need to set up some prerequisites:\n",
        "<br>\n",
        "- **Pre-trained Model***: We will use the pre-trained model ***Salesforce/xgen-7b-8k-base***, which is available on **Hugging Face**. Salesforce trained this series of 7B LLMs named XGen-7B with standard dense attention on up to 8K sequences for up to 1.5T tokens.\n",
        "- **Tokenizer**: This is needed for tokenization tasks on the training data. The code to load the pre-trained model and tokenizer is as follows:\n",
        "\n",
        "- The first line sets the variable ***pretrained_model_name*** to the string \"Salesforce/xgen-7b-8k-base\".\n",
        "- The second line uses the ***from_pretrained method*** of ***AutoModelForCausalLM*** to load a pretrained model.\n",
        "- The pretrained model is specified by the ***pretrained_model_name***, and the data type for the model's tensors is set to **torch.bfloat16**.\n",
        "- The third line uses the ***from_pretrained*** method of **AutoTokenizer** to load a tokenizer that matches the pretrained model.\n",
        "- The tokenizer is also specified by the ***pretrained_model_name***, and the ***trust_remote_code*** parameter is set to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc0cd17",
      "metadata": {
        "id": "7bc0cd17",
        "outputId": "afc1e77d-a64b-4d32-a783-59b13dee5e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/sgangoly/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from tiktoken) (2023.3.23)\n",
            "Requirement already satisfied: requests>=2.26.0 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from tiktoken) (2.28.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/tf/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea70ea77",
      "metadata": {
        "id": "ea70ea77",
        "outputId": "197c7de3-1b06-4d23-83f8-15d7bec63bc9",
        "colab": {
          "referenced_widgets": [
            "62f5f86fb681403a937f9e2b235a57bb"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62f5f86fb681403a937f9e2b235a57bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretrained_model_name = \"Salesforce/xgen-7b-8k-base\"\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca13f933",
      "metadata": {
        "id": "ca13f933"
      },
      "source": [
        "### Training configuration ###\n",
        "The training requires some training arguments and configurations, and the two important configuration objects are defined below, an instance of the **TrainingArguments**, an instance of the **LoraConfig model**, and finally the **SFTTrainer** model.\n",
        "\n",
        "**TrainingArguments** : This is used to define the parameters for model training.\n",
        "\n",
        "In this specific scenario, we start by defining the destination where the trained model will be stored using the *output_dir* attribute before defining additional hyperparameters, such as the *optimization method*, the *learning rate*, the *number of epochs*, and more.\n",
        "<br>\n",
        "\n",
        "- This code is creating an instance of the TrainingArguments class with specific parameters.\n",
        "- **output_dir=\"xgen-7b-8k-base-fine-tuned\"** sets the directory where the model and its outputs will be saved.\n",
        "- **per_device_train_batch_size=4** sets the number of training examples utilized in one iteration.\n",
        "- **optim=\"adamw_torch\"** sets the optimizer to be used during training to AdamW from the PyTorch library.\n",
        "- **logging_steps=80** sets the frequency of logging steps, i.e., logs will be printed every 80 steps.\n",
        "- **learning_rate=2e-4** sets the learning rate for the optimizer.\n",
        "- **warmup_ratio=0.1** sets the ratio of total steps for which linear warmup will be performed.\n",
        "- **lr_scheduler_type=\"linear\"** sets the learning rate scheduler to linear.\n",
        "- **num_train_epochs=1** sets the number of times the learning algorithm will work through the entire training dataset.\n",
        "- **save_strategy=\"epoch\"** sets the strategy to save checkpoints, in this case, at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baae3c73",
      "metadata": {
        "id": "baae3c73"
      },
      "outputs": [],
      "source": [
        "model_training_args = TrainingArguments(\n",
        "       output_dir=\"xgen-7b-8k-base-fine-tuned\",\n",
        "       per_device_train_batch_size=4,\n",
        "       optim=\"adamw_torch\",\n",
        "       logging_steps=80,\n",
        "       learning_rate=2e-4,\n",
        "       warmup_ratio=0.1,\n",
        "       lr_scheduler_type=\"linear\",\n",
        "       num_train_epochs=1,\n",
        "       save_strategy=\"epoch\"\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6610c24",
      "metadata": {
        "id": "f6610c24"
      },
      "source": [
        "### LoRAConfig ###\n",
        "The main arguments used for this scenario are the rank of the low-rank transformation matrix in LoRA, which is set to 16. Then, the scaling factor for the additional parameters in LoRA is set to 32.\n",
        "<br>\n",
        "The dropout ratio is 0.05, meaning that 5% of the input units will be ignored during the training. Finally, since we are dealing with a causual language modeling, the task is hence initialized with the **CAUSAL_LM attribute**.\n",
        "\n",
        "### SFTTrainer ###\n",
        "This aims to train the model using the training data, the tokenizer, and additional information such as the above models.\n",
        "\n",
        "Since we are using the text field from the training data, it is important to have a look at the distribution in order to help in setting the maximum number of tokens in a given sequence.\n",
        "<br>\n",
        "\n",
        "- The code starts by importing the matplotlib.pyplot library, which is used for data visualization.\n",
        "- The second line adds a new column 'text_length' to the pandas dataframe 'pandas_format'.\n",
        "- This column contains the length of each text in the 'text' column.\n",
        "- 'plt.figure(figsize=(10,6))' sets the size of the figure to be created.\n",
        "- 'plt.hist()' creates a histogram of the 'text_length' column with 50 bins, semi-transparent and green in color.\n",
        "- 'plt.title()', 'plt.xlabel()', and 'plt.ylabel()' label the plot's title, x-axis, and y-axis respectively.\n",
        "- 'plt.grid(True)' adds a grid to the plot for better readability.\n",
        "- Finally, 'plt.show()' displays the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1324f9",
      "metadata": {
        "id": "8a1324f9",
        "outputId": "170c6c2b-4b2b-4b4b-abcc-78ef283fa2d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "pandas_format['text_length'] = pandas_format['text'].apply(len)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(pandas_format['text_length'], bins=50, alpha=0.5, color='g')\n",
        "plt.title('Distribution of Length of Text')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b4225f",
      "metadata": {
        "id": "a7b4225f"
      },
      "source": [
        "Based on the above observation, we can see that the majority of text has a length between 0 and 1000. Also, we can see below that only 4.5% percent of the text documents have a length greater than 1024.\n",
        "\n",
        "<br>\n",
        "\n",
        "- The code uses the pandas library to analyze a DataFrame named 'pandas_format'.\n",
        "- 'mask' is a boolean Series where each value is True if the corresponding 'text_length' is greater than 1024.\n",
        "- 'mask.sum()' counts the number of True values (i.e., text lengths greater than 1024).\n",
        "- 'pandas_format['text_length'].count()' gets the total number of 'text_length' entries in the DataFrame.\n",
        "- 'percentage' calculates the percentage of 'text_length' values that are greater than 1024.\n",
        "- The final line prints this percentage, formatted as a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65224992",
      "metadata": {
        "id": "65224992",
        "outputId": "435eaef7-77d3-450d-94fe-9313625d2f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The percentage of text documents with a length greater than 1024 is: 4.499826929733472%\n"
          ]
        }
      ],
      "source": [
        "mask = pandas_format['text_length'] > 1024\n",
        "percentage = (mask.sum() / pandas_format['text_length'].count()) * 100\n",
        "\n",
        "\n",
        "print(f\"The percentage of text documents with a length greater than 1024 is: {percentage}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6b1077",
      "metadata": {
        "id": "bc6b1077"
      },
      "source": [
        "Then we set the maximum number of tokens in the sequence to 1024 so that any text longer than this get truncated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d697a3b",
      "metadata": {
        "id": "2d697a3b"
      },
      "source": [
        "- This code snippet creates an instance of the SFTTrainer class.\n",
        "- **model=model** is passing a pre-defined model to the trainer.\n",
        "- **train_dataset=train_dataset** is passing the training data to the trainer.\n",
        "- **dataset_text_field=\"text\"** specifies the field in the dataset that contains the text data.\n",
        "- **max_seq_length=1024** sets the maximum sequence length for the model's input data.\n",
        "- **tokenizer=tokenizer** is passing a pre-defined tokenizer to the trainer.\n",
        "- **args=model_training_args** is passing a set of arguments for model training.\n",
        "- **packing=True** enables packing of the sequences for efficient training.\n",
        "- **peft_config=lora_peft_config** is passing a configuration for the PEFT algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd87e13",
      "metadata": {
        "id": "2cd87e13",
        "outputId": "5f66c18c-958b-4d9d-90c5-2d477d03752b",
        "colab": {
          "referenced_widgets": [
            "14951c77e1f945ba8a9844391b321d51"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14951c77e1f945ba8a9844391b321d51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 3.81 GiB total capacity; 3.15 GiB already allocated; 22.62 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lora_peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m SFT_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_training_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_peft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:287\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    282\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     )\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m packing:\n\u001b[1;32m    302\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:456\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    455\u001b[0m ):\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:690\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 690\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (4 times)]\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 3.81 GiB total capacity; 3.15 GiB already allocated; 22.62 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "lora_peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\")\n",
        "\n",
        "SFT_trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=1024,\n",
        "        tokenizer=tokenizer,\n",
        "        args=model_training_args,\n",
        "        packing=True,\n",
        "        peft_config=lora_peft_config,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64816175",
      "metadata": {
        "id": "64816175"
      },
      "source": [
        "### Training execution ###\n",
        "With all the prerequisites in place, we can now run the training process of the model as follows:\n",
        "- The first line sets the padding token of the tokenizer to be the same as the end-of-sentence (eos) token.\n",
        "- The second line resizes the token embeddings of the model to match the length of the tokenizer's vocabulary.\n",
        "- The third line prepares the model for INT8 training, which is a form of quantization that reduces memory usage.\n",
        "- The fourth line gets a model that is configured for PEFT (Performance Evaluation and Forecasting Tool) using a specific configuration.\n",
        "- The fifth line assigns the model training arguments to a variable called training_args.\n",
        "- The sixth line assigns the SFT (Supervised Fine-Tuning) trainer to a variable called trainer.\n",
        "- The final line calls the train method of the trainer to start the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675cd9ff",
      "metadata": {
        "id": "675cd9ff",
        "outputId": "1c29b706-1675-43a1-dcf9-0da372adf958"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sgangoly/anaconda3/envs/tf/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB (GPU 0; 3.81 GiB total capacity; 3.15 GiB already allocated; 22.62 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_for_int8_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, lora_peft_config)\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m model_training_args\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/peft/utils/other.py:145\u001b[0m, in \u001b[0;36mprepare_model_for_int8_training\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_model_for_int8_training\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m     )\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/peft/utils/other.py:103\u001b[0m, in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m--> 103\u001b[0m             param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (loaded_in_kbit \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 800.00 MiB (GPU 0; 3.81 GiB total capacity; 3.15 GiB already allocated; 22.62 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = prepare_model_for_int8_training(model)\n",
        "model = get_peft_model(model, lora_peft_config)\n",
        "training_args = model_training_args\n",
        "trainer = SFT_trainer\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b836cdd4",
      "metadata": {
        "id": "b836cdd4"
      },
      "source": [
        "It is important to mention that this training was performed on a local computer with a RTX 3070 GPU. It would likely be much faster on a cloud environment with a GPU.\n",
        "<br>\n",
        "\n",
        "- **tokenizer.pad_token = tokenizer.eos_token**: Sets padding token to be the same as the end-of-sentence token.\n",
        "- **model.resize_token_embeddings(len(tokenizer))**: Resizes the token embedding layer of the model to match the length of the tokenizer vocabulary.\n",
        "- **model = prepare_model_for_int8_training(model)**: Prepares the model for training with INT8 precision, likely performing quantization.\n",
        "- **model = get_peft_model(model, lora_peft_config)**: Adjusts the given model according to the PEFT configuration.\n",
        "- **training_args = model_training_args**: Assigns predefined training arguments to training_args.\n",
        "- **trainer = SFT_trainer**: Assigns the SFTTrainer instance to the variable trainer.\n",
        "- **trainer.train()**: Triggers the training process of the model according to the provided specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14004733",
      "metadata": {
        "id": "14004733"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}